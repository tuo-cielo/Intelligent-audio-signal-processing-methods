{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379179bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== 1. БАЗОВЫЕ КОМПОНЕНТЫ ====================\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    Кастомная реализация линейного слоя\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        \n",
    "        # Инициализация смещения\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Инициализация параметров по методу Xavier/Glorot\"\"\"\n",
    "        # Более стабильная инициализация\n",
    "        nn.init.xavier_uniform_(self.weight, gain=1.0)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Прямой проход\"\"\"\n",
    "        # Поддержка многомерных тензоров\n",
    "        if x.dim() > 2:\n",
    "            # Сохраняем форму\n",
    "            original_shape = x.shape\n",
    "            # Выпрямляем все кроме последнего измерения\n",
    "            x = x.reshape(-1, original_shape[-1])\n",
    "            output = torch.matmul(x, self.weight.t())\n",
    "            if self.bias is not None:\n",
    "                output += self.bias\n",
    "            # Восстанавливаем форму\n",
    "            output = output.reshape(*original_shape[:-1], self.out_features)\n",
    "        else:\n",
    "            output = torch.matmul(x, self.weight.t())\n",
    "            if self.bias is not None:\n",
    "                output += self.bias\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n",
    "\n",
    "class ReLU(nn.Module):\n",
    "    \"\"\"Функция активации ReLU\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.relu(x)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return ''\n",
    "\n",
    "class LeakyReLU(nn.Module):\n",
    "    \"\"\"Функция активации LeakyReLU\"\"\"\n",
    "    def __init__(self, negative_slope: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.negative_slope = negative_slope\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.leaky_relu(x, negative_slope=self.negative_slope)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'negative_slope={self.negative_slope}'\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    \"\"\"Сигмоидальная функция активации\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    \"\"\"Гиперболический тангенс\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    \"\"\"Функция Softmax\"\"\"\n",
    "    def __init__(self, dim: int = -1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.softmax(x, dim=self.dim)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'dim={self.dim}'\n",
    "\n",
    "# ==================== 2. AUTOENCODER ДЛЯ ШУМОПОДАВЛЕНИЯ ====================\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Автоэнкодер для задачи шумоподавления\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 784, hidden_dims: List[int] = [512, 256, 128]):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Энкодер\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.append(Linear(prev_dim, hidden_dim))\n",
    "            encoder_layers.append(ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Декодер (симметричная архитектура)\n",
    "        decoder_layers = []\n",
    "        hidden_dims_rev = hidden_dims[::-1]\n",
    "        prev_dim = hidden_dims_rev[0]\n",
    "        \n",
    "        for hidden_dim in hidden_dims_rev[1:]:\n",
    "            decoder_layers.append(Linear(prev_dim, hidden_dim))\n",
    "            decoder_layers.append(ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Выходной слой\n",
    "        decoder_layers.append(Linear(prev_dim, input_dim))\n",
    "        decoder_layers.append(Sigmoid())\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Прямой проход\"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "    \n",
    "    def add_noise(self, x, noise_level: float = 0.3):\n",
    "        \"\"\"Добавление гауссовского шума\"\"\"\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        noisy_x = x + noise\n",
    "        return torch.clamp(noisy_x, 0.0, 1.0)\n",
    "\n",
    "# ==================== 3. VARIATIONAL AUTOENCODER ====================\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Вариационный автоэнкодер\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 784, hidden_dim: int = 400, latent_dim: int = 20):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Энкодер\n",
    "        self.encoder = nn.Sequential(\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU()\n",
    "        )\n",
    "        \n",
    "        # Слои для параметров распределения\n",
    "        self.fc_mu = Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Декодер\n",
    "        self.decoder = nn.Sequential(\n",
    "            Linear(latent_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, input_dim),\n",
    "            Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Кодирование в параметры распределения\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Репараметризация для обратного распространения\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Декодирование из латентного пространства\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Прямой проход\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "    \n",
    "    def sample(self, num_samples: int, device: str = 'cpu'):\n",
    "        \"\"\"Генерация новых семплов\"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim).to(device)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "def vae_loss(reconstructed, original, mu, logvar, beta: float = 1.0):\n",
    "    \"\"\"\n",
    "    Функция потерь для VAE (ELBO)\n",
    "    \"\"\"\n",
    "    # Reconstruction loss\n",
    "    recon_loss = F.binary_cross_entropy(reconstructed, original, reduction='mean') * original.size(0)\n",
    "    \n",
    "    # KL divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = recon_loss + beta * kld_loss\n",
    "    \n",
    "    return total_loss, recon_loss, kld_loss\n",
    "\n",
    "# ==================== 4. GENERATIVE ADVERSARIAL NETWORK ====================\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Генератор для GAN\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int = 100, hidden_dim: int = 256, output_dim: int = 784):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            Linear(latent_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, output_dim),\n",
    "            Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Дискриминатор для GAN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 784, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            LeakyReLU(0.2),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            LeakyReLU(0.2),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            LeakyReLU(0.2),\n",
    "            Linear(hidden_dim, 1),\n",
    "            Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Полная GAN модель\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int = 100, hidden_dim: int = 256, output_dim: int = 784):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(latent_dim, hidden_dim, output_dim)\n",
    "        self.discriminator = Discriminator(output_dim, hidden_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def generate(self, num_samples: int, device: str = 'cpu'):\n",
    "        \"\"\"Генерация новых семплов\"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim).to(device)\n",
    "        return self.generator(z)\n",
    "\n",
    "# ==================== 5. DENOISING DIFFUSION PROBABILISTIC MODEL ====================\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Упрощенная DDPM модель\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 784, hidden_dim: int = 512, timesteps: int = 100):\n",
    "        super().__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Параметры диффузионного процесса\n",
    "        self.register_buffer('betas', self._linear_beta_schedule(timesteps))\n",
    "        self.register_buffer('alphas', 1. - self.betas)\n",
    "        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - self.alphas_cumprod))\n",
    "        \n",
    "        # Нейросеть для предсказания шума\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            Linear(1, 16),\n",
    "            ReLU(),\n",
    "            Linear(16, 32),\n",
    "            ReLU(),\n",
    "            Linear(32, 16)\n",
    "        )\n",
    "        \n",
    "        self.denoiser = nn.Sequential(\n",
    "            Linear(input_dim + 16, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def _linear_beta_schedule(self, timesteps, start=0.0001, end=0.02):\n",
    "        \"\"\"Линейное расписание beta\"\"\"\n",
    "        return torch.linspace(start, end, timesteps)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Прямой проход для предсказания шума\"\"\"\n",
    "        # Эмбеддинг временного шага\n",
    "        t_float = t.float().unsqueeze(-1) / self.timesteps\n",
    "        t_embed = self.time_embedding(t_float)\n",
    "        \n",
    "        # Конкатенация с входными данными\n",
    "        x_with_t = torch.cat([x, t_embed], dim=-1)\n",
    "        \n",
    "        # Предсказание шума\n",
    "        predicted_noise = self.denoiser(x_with_t)\n",
    "        return predicted_noise\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"Прямой диффузионный процесс (добавление шума)\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alpha_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1)\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)\n",
    "        \n",
    "        return sqrt_alpha_cumprod_t * x_start + sqrt_one_minus_alpha_cumprod_t * noise\n",
    "    \n",
    "    def p_sample(self, x, t):\n",
    "        \"\"\"Обратный диффузионный процесс (удаление шума)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            predicted_noise = self.forward(x, t)\n",
    "            \n",
    "            alpha_t = self.alphas[t].view(-1, 1)\n",
    "            sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)\n",
    "            \n",
    "            # Формула обратного процесса\n",
    "            x_prev = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x - (self.betas[t].view(-1, 1) / sqrt_one_minus_alpha_cumprod_t) * predicted_noise\n",
    "            )\n",
    "            \n",
    "            if t[0] > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                x_prev += torch.sqrt(self.betas[t].view(-1, 1)) * noise\n",
    "            \n",
    "            return x_prev\n",
    "    \n",
    "    def generate(self, num_samples, img_shape, device='cpu'):\n",
    "        \"\"\"Генерация новых изображений\"\"\"\n",
    "        # Начинаем с чистого шума\n",
    "        x = torch.randn(num_samples, *img_shape).to(device)\n",
    "        \n",
    "        # Обратный процесс\n",
    "        for t in tqdm(reversed(range(self.timesteps)), desc=\"Sampling\"):\n",
    "            t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t_batch)\n",
    "        \n",
    "        return torch.clamp(x, -1.0, 1.0)\n",
    "\n",
    "# ==================== 6. AUTOREGRESSIVE MODEL ====================\n",
    "\n",
    "class MaskedLinear(Linear):\n",
    "    \"\"\"\n",
    "    Маскированный линейный слой для автогрессивных моделей\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, mask_type='A', bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        \n",
    "        # Создание маски\n",
    "        mask = torch.zeros(out_features, in_features)\n",
    "        \n",
    "        # Простая маска: разрешаем связи только с предыдущими пикселями\n",
    "        for i in range(out_features):\n",
    "            # Разрешаем связи со всеми предыдущими пикселями\n",
    "            if mask_type == 'A':\n",
    "                # Маска типа A: запрет связей с текущим и будущими пикселями\n",
    "                mask[i, :min(i, in_features)] = 1\n",
    "            else:\n",
    "                # Маска типа B: разрешение связей с текущим пикселем\n",
    "                mask[i, :min(i+1, in_features)] = 1\n",
    "        \n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Применяем маску к весам перед умножением\n",
    "        masked_weight = self.weight * self.mask\n",
    "        return F.linear(x, masked_weight, self.bias)\n",
    "\n",
    "class AutoregressiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Автогрессивная модель (PixelCNN-like)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=256, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Упрощенная архитектура\n",
    "        layers = []\n",
    "        # Первый слой\n",
    "        layers.append(MaskedLinear(input_dim, hidden_dim, mask_type='A'))\n",
    "        layers.append(ReLU())\n",
    "        \n",
    "        # Промежуточные слои\n",
    "        for _ in range(n_layers - 2):\n",
    "            layers.append(MaskedLinear(hidden_dim, hidden_dim, mask_type='B'))\n",
    "            layers.append(ReLU())\n",
    "        \n",
    "        # Выходной слой\n",
    "        layers.append(MaskedLinear(hidden_dim, input_dim, mask_type='B'))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Предсказание logits для каждого пикселя\"\"\"\n",
    "        return self.net(x)\n",
    "    \n",
    "    def generate(self, num_samples, device='cpu'):\n",
    "        \"\"\"Последовательная генерация\"\"\"\n",
    "        samples = torch.zeros(num_samples, self.input_dim).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(self.input_dim):\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Generating pixel {i}/{self.input_dim}\")\n",
    "                \n",
    "                logits = self.forward(samples)\n",
    "                probs = torch.sigmoid(logits[:, i:i+1])\n",
    "                \n",
    "                # Сэмплирование следующего пикселя\n",
    "                samples[:, i:i+1] = torch.bernoulli(probs)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "# ==================== 7. УТИЛИТЫ ДЛЯ ОБУЧЕНИЯ И ВИЗУАЛИЗАЦИИ ====================\n",
    "\n",
    "def train_denoising_ae(model, train_loader, test_loader, epochs=20, lr=1e-3, device='cpu'):\n",
    "    \"\"\"Обучение автоэнкодера для шумоподавления\"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Обучение\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)):\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            \n",
    "            # Добавление шума\n",
    "            noisy_data = model.add_noise(data, noise_level=0.3)\n",
    "            \n",
    "            # Прямой проход\n",
    "            reconstructed = model(noisy_data)\n",
    "            \n",
    "            # Вычисление лосса\n",
    "            loss = criterion(reconstructed, data)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Градиентный клиппинг для стабильности\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.view(data.size(0), -1).to(device)\n",
    "                noisy_data = model.add_noise(data, noise_level=0.3)\n",
    "                reconstructed = model(noisy_data)\n",
    "                loss = criterion(reconstructed, data)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Test Loss = {test_loss:.6f}')\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "def train_vae(model, train_loader, test_loader, epochs=30, lr=1e-3, device='cpu', beta=0.001):\n",
    "    \"\"\"Обучение VAE\"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    train_losses = []\n",
    "    recon_losses = []\n",
    "    kld_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_recon = 0\n",
    "        total_kld = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)):\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            \n",
    "            # Прямой проход\n",
    "            reconstructed, mu, logvar = model(data)\n",
    "            \n",
    "            # Вычисление лосса\n",
    "            loss, recon_loss, kld_loss = vae_loss(reconstructed, data, mu, logvar, beta)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            total_kld += kld_loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        avg_recon = total_recon / len(train_loader.dataset)\n",
    "        avg_kld = total_kld / len(train_loader.dataset)\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        recon_losses.append(avg_recon)\n",
    "        kld_losses.append(avg_kld)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss = {avg_loss:.6f}, Recon = {avg_recon:.6f}, KLD = {avg_kld:.6f}')\n",
    "    \n",
    "    return train_losses, recon_losses, kld_losses\n",
    "\n",
    "def train_gan(gan_model, train_loader, epochs=50, lr=1e-4, device='cpu'):\n",
    "    \"\"\"Обучение GAN\"\"\"\n",
    "    generator = gan_model.generator.to(device)\n",
    "    discriminator = gan_model.discriminator.to(device)\n",
    "    \n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        g_loss_epoch = 0\n",
    "        d_loss_epoch = 0\n",
    "        \n",
    "        for batch_idx, (real_data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)):\n",
    "            batch_size = real_data.size(0)\n",
    "            real_data = real_data.view(batch_size, -1).to(device)\n",
    "            \n",
    "            # Нормализация в диапазон [-1, 1]\n",
    "            real_data = (real_data - 0.5) * 2\n",
    "            \n",
    "            # Метки для дискриминатора (с шумом для стабильности)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device) * 0.9  # Label smoothing\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device) + 0.1  # Label smoothing\n",
    "            \n",
    "            # ========== Обучение дискриминатора ==========\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # Реальные данные\n",
    "            real_output = discriminator(real_data)\n",
    "            d_real_loss = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Сгенерированные данные\n",
    "            z = torch.randn(batch_size, gan_model.latent_dim).to(device)\n",
    "            fake_data = generator(z)\n",
    "            fake_output = discriminator(fake_data.detach())\n",
    "            d_fake_loss = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            # Общий лосс дискриминатора\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # ========== Обучение генератора ==========\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, gan_model.latent_dim).to(device)\n",
    "            fake_data = generator(z)\n",
    "            fake_output = discriminator(fake_data)\n",
    "            \n",
    "            # Генератор хочет, чтобы дискриминатор считал фейковые данные настоящими\n",
    "            g_loss = criterion(fake_output, real_labels)\n",
    "            g_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_loss_epoch += g_loss.item()\n",
    "            d_loss_epoch += d_loss.item()\n",
    "        \n",
    "        g_losses.append(g_loss_epoch / len(train_loader))\n",
    "        d_losses.append(d_loss_epoch / len(train_loader))\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: G Loss = {g_losses[-1]:.6f}, D Loss = {d_losses[-1]:.6f}')\n",
    "        \n",
    "        # Сохранение лучшей модели GAN\n",
    "        if epoch > 20 and g_losses[-1] < min(g_losses[:-1]):\n",
    "            torch.save(gan_model.state_dict(), 'gan_best_model.pth')\n",
    "            print(f\"Saved best GAN model at epoch {epoch+1}\")\n",
    "    \n",
    "    return g_losses, d_losses\n",
    "\n",
    "def train_diffusion(model, train_loader, epochs=30, lr=1e-4, device='cpu'):\n",
    "    \"\"\"Обучение диффузионной модели\"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)):\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            \n",
    "            # Нормализация в диапазон [-1, 1]\n",
    "            data = (data - 0.5) * 2\n",
    "            \n",
    "            batch_size = data.size(0)\n",
    "            \n",
    "            # Выбор случайного временного шага\n",
    "            t = torch.randint(0, model.timesteps, (batch_size,), device=device).long()\n",
    "            \n",
    "            # Генерация шума\n",
    "            noise = torch.randn_like(data)\n",
    "            \n",
    "            # Добавление шума (прямой процесс)\n",
    "            noisy_data = model.q_sample(data, t, noise)\n",
    "            \n",
    "            # Предсказание шума\n",
    "            predicted_noise = model(noisy_data, t)\n",
    "            \n",
    "            # Вычисление лосса\n",
    "            loss = F.mse_loss(predicted_noise, noise)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss = {avg_loss:.6f}')\n",
    "        \n",
    "        # Сохранение лучшей модели Diffusion\n",
    "        if epoch > 10 and losses[-1] < min(losses[:-1]):\n",
    "            torch.save(model.state_dict(), 'diffusion_best_model.pth')\n",
    "            print(f\"Saved best Diffusion model at epoch {epoch+1}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_autoregressive(model, train_loader, epochs=20, lr=1e-3, device='cpu'):\n",
    "    \"\"\"Обучение автогрессивной модели\"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)):\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            \n",
    "            # Прямой проход\n",
    "            logits = model(data)\n",
    "            \n",
    "            # Вычисление лосса (бинарная классификация для каждого пикселя)\n",
    "            loss = criterion(logits, data)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss = {avg_loss:.6f}')\n",
    "        \n",
    "        # Сохранение лучшей модели Autoregressive\n",
    "        if epoch > 5 and losses[-1] < min(losses[:-1]):\n",
    "            torch.save(model.state_dict(), 'ar_best_model.pth')\n",
    "            print(f\"Saved best Autoregressive model at epoch {epoch+1}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def visualize_results(model, test_loader, model_type='ae', device='cpu', gan_model=None):\n",
    "    \"\"\"Визуализация результатов для разных моделей\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Получение тестовых данных\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data.view(data.size(0), -1).to(device)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    if model_type == 'ae':\n",
    "        # Для автоэнкодера: шум -> восстановление\n",
    "        noisy_data = model.add_noise(data[:10], noise_level=0.3)\n",
    "        reconstructed = model(noisy_data)\n",
    "        \n",
    "        for i in range(10):\n",
    "            # Original\n",
    "            ax = fig.add_subplot(3, 10, i+1)\n",
    "            ax.imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Original')\n",
    "            \n",
    "            # Noisy\n",
    "            ax = fig.add_subplot(3, 10, i+11)\n",
    "            ax.imshow(noisy_data[i].cpu().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Noisy')\n",
    "            \n",
    "            # Reconstructed\n",
    "            ax = fig.add_subplot(3, 10, i+21)\n",
    "            ax.imshow(reconstructed[i].cpu().detach().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Reconstructed')\n",
    "    \n",
    "    elif model_type == 'vae':\n",
    "        # Для VAE: реконструкция и генерация\n",
    "        reconstructed, mu, logvar = model(data[:10])\n",
    "        samples = model.sample(10, device)\n",
    "        \n",
    "        for i in range(10):\n",
    "            # Original\n",
    "            ax = fig.add_subplot(3, 10, i+1)\n",
    "            ax.imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Original')\n",
    "            \n",
    "            # Reconstructed\n",
    "            ax = fig.add_subplot(3, 10, i+11)\n",
    "            ax.imshow(reconstructed[i].cpu().detach().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Reconstructed')\n",
    "            \n",
    "            # Generated\n",
    "            ax = fig.add_subplot(3, 10, i+21)\n",
    "            ax.imshow(samples[i].cpu().detach().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Generated')\n",
    "    \n",
    "    elif model_type == 'gan':\n",
    "        # Для GAN:\n",
    "        if gan_model is None:\n",
    "            gan_model = model\n",
    "        samples = gan_model.generate(10, device)\n",
    "        samples = (samples + 1) / 2  # Денормализация из [-1, 1] в [0, 1]\n",
    "        \n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i+1)\n",
    "            ax.imshow(samples[i].cpu().detach().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Generated')\n",
    "    \n",
    "    elif model_type == 'diffusion':\n",
    "        # Для диффузионной модели: генерация\n",
    "        samples = model.generate(10, (784,), device)\n",
    "        samples = (samples + 1) / 2  # Денормализация\n",
    "        \n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i+1)\n",
    "            ax.imshow(samples[i].cpu().detach().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Generated')\n",
    "    \n",
    "    elif model_type == 'ar':\n",
    "        # Для автогрессивной модели: генерация\n",
    "        print(\"Generating autoregressive samples...\")\n",
    "        samples = model.generate(10, device)\n",
    "        \n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i+1)\n",
    "            ax.imshow(samples[i].cpu().detach().view(28, 28), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            if i == 0:\n",
    "                ax.set_title('Generated')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==================== 8. ОСНОВНОЙ БЛОК ДЛЯ ЗАПУСКА ====================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Основная функция для обучения и оценки всех моделей\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"КУРСОВАЯ РАБОТА: ГЕНЕРАТИВНЫЕ МОДЕЛИ (УВЕЛИЧЕННОЕ КОЛИЧЕСТВО ЭПОХ)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Проверка доступности GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Используемое устройство: {device}\")\n",
    "    \n",
    "    # Загрузка данных (MNIST)\n",
    "    print(\"\\nЗагрузка данных MNIST...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Увеличили batch size\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"Размер обучающей выборки: {len(train_dataset)}\")\n",
    "    print(f\"Размер тестовой выборки: {len(test_dataset)}\")\n",
    "    \n",
    "    # ========== 1. AUTOENCODER ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"1. ОБУЧЕНИЕ AUTOENCODER ДЛЯ ШУМОПОДАВЛЕНИЯ (20 эпох)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ae_model = DenoisingAutoencoder(input_dim=784, hidden_dims=[512, 256, 128])\n",
    "    print(f\"Параметры модели: {sum(p.numel() for p in ae_model.parameters()):,}\")\n",
    "    \n",
    "    ae_train_losses, ae_test_losses = train_denoising_ae(\n",
    "        ae_model, train_loader, test_loader, epochs=20, lr=1e-3, device=device\n",
    "    )\n",
    "    \n",
    "    visualize_results(ae_model, test_loader, model_type='ae', device=device)\n",
    "    \n",
    "    # ========== 2. VARIATIONAL AUTOENCODER ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"2. ОБУЧЕНИЕ VARIATIONAL AUTOENCODER (30 эпох)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    vae_model = VariationalAutoencoder(input_dim=784, hidden_dim=400, latent_dim=20)\n",
    "    print(f\"Параметры модели: {sum(p.numel() for p in vae_model.parameters()):,}\")\n",
    "    \n",
    "    vae_losses, recon_losses, kld_losses = train_vae(\n",
    "        vae_model, train_loader, test_loader, epochs=30, lr=1e-3, device=device, beta=0.001\n",
    "    )\n",
    "    \n",
    "    visualize_results(vae_model, test_loader, model_type='vae', device=device)\n",
    "    \n",
    "    # ========== 3. GENERATIVE ADVERSARIAL NETWORK ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"3. ОБУЧЕНИЕ GENERATIVE ADVERSARIAL NETWORK (50 эпох)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    gan_model = GAN(latent_dim=100, hidden_dim=256, output_dim=784)\n",
    "    print(f\"Параметры генератора: {sum(p.numel() for p in gan_model.generator.parameters()):,}\")\n",
    "    print(f\"Параметры дискриминатора: {sum(p.numel() for p in gan_model.discriminator.parameters()):,}\")\n",
    "    \n",
    "    g_losses, d_losses = train_gan(\n",
    "        gan_model, train_loader, epochs=50, lr=1e-4, device=device\n",
    "    )\n",
    "    \n",
    "    visualize_results(gan_model, test_loader, model_type='gan', device=device)\n",
    "    \n",
    "    # ========== 4. DIFFUSION MODEL ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"4. ОБУЧЕНИЕ DIFFUSION MODEL (30 эпох)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    diffusion_model = DiffusionModel(input_dim=784, hidden_dim=512, timesteps=50)\n",
    "    print(f\"Параметры модели: {sum(p.numel() for p in diffusion_model.parameters()):,}\")\n",
    "    \n",
    "    diffusion_losses = train_diffusion(\n",
    "        diffusion_model, train_loader, epochs=30, lr=1e-4, device=device\n",
    "    )\n",
    "    \n",
    "    visualize_results(diffusion_model, test_loader, model_type='diffusion', device=device)\n",
    "    \n",
    "    # ========== 5. AUTOREGRESSIVE MODEL ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"5. ОБУЧЕНИЕ AUTOREGRESSIVE MODEL (20 эпох)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ar_model = AutoregressiveModel(input_dim=784, hidden_dim=256, n_layers=4)\n",
    "    print(f\"Параметры модели: {sum(p.numel() for p in ar_model.parameters()):,}\")\n",
    "    \n",
    "    ar_losses = train_autoregressive(\n",
    "        ar_model, train_loader, epochs=20, lr=1e-3, device=device\n",
    "    )\n",
    "    \n",
    "    visualize_results(ar_model, test_loader, model_type='ar', device=device)\n",
    "    \n",
    "    # ========== ВИЗУАЛИЗАЦИЯ КРИВЫХ ОБУЧЕНИЯ ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ВИЗУАЛИЗАЦИЯ КРИВЫХ ОБУЧЕНИЯ ВСЕХ МОДЕЛЕЙ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # AE\n",
    "    axes[0, 0].plot(ae_train_losses, label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(ae_test_losses, label='Test', linewidth=2)\n",
    "    axes[0, 0].set_title('Autoencoder (MSE Loss)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].text(0.7, 0.9, f'Final: {ae_test_losses[-1]:.6f}', \n",
    "                   transform=axes[0, 0].transAxes, fontsize=10)\n",
    "    \n",
    "    # VAE\n",
    "    axes[0, 1].plot(vae_losses, label='Total Loss', linewidth=2)\n",
    "    axes[0, 1].plot(recon_losses, label='Reconstruction', linewidth=2)\n",
    "    axes[0, 1].plot(kld_losses, label='KLD', linewidth=2)\n",
    "    axes[0, 1].set_title('Variational Autoencoder', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # GAN\n",
    "    axes[0, 2].plot(g_losses, label='Generator', linewidth=2)\n",
    "    axes[0, 2].plot(d_losses, label='Discriminator', linewidth=2)\n",
    "    axes[0, 2].set_title('GAN Losses', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Loss')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].text(0.7, 0.9, f'Final G: {g_losses[-1]:.4f}\\nFinal D: {d_losses[-1]:.4f}', \n",
    "                   transform=axes[0, 2].transAxes, fontsize=9)\n",
    "    \n",
    "    # Diffusion\n",
    "    axes[1, 0].plot(diffusion_losses, linewidth=2, color='green')\n",
    "    axes[1, 0].set_title('Diffusion Model (MSE Loss)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].text(0.7, 0.9, f'Final: {diffusion_losses[-1]:.4f}', \n",
    "                   transform=axes[1, 0].transAxes, fontsize=10)\n",
    "    \n",
    "    # AR\n",
    "    axes[1, 1].plot(ar_losses, linewidth=2, color='purple')\n",
    "    axes[1, 1].set_title('Autoregressive Model (BCE Loss)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].text(0.7, 0.9, f'Final: {ar_losses[-1]:.4f}', \n",
    "                   transform=axes[1, 1].transAxes, fontsize=10)\n",
    "    \n",
    "    # Сводная таблица\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    # Расчет улучшений\n",
    "    ae_improvement = ((ae_train_losses[0] - ae_train_losses[-1]) / ae_train_losses[0] * 100) if ae_train_losses[0] > 0 else 0\n",
    "    vae_improvement = ((vae_losses[0] - vae_losses[-1]) / vae_losses[0] * 100) if vae_losses[0] > 0 else 0\n",
    "    diff_improvement = ((diffusion_losses[0] - diffusion_losses[-1]) / diffusion_losses[0] * 100) if diffusion_losses[0] > 0 else 0\n",
    "    ar_improvement = ((ar_losses[0] - ar_losses[-1]) / ar_losses[0] * 100) if ar_losses[0] > 0 else 0\n",
    "    \n",
    "    summary_text = (\n",
    "        \"ИТОГОВАЯ СВОДКА ПОСЛЕ УВЕЛИЧЕНИЯ ЭПОХ:\\n\\n\"\n",
    "        f\"Autoencoder (20 эпох):\\n\"\n",
    "        f\"  Train Loss: {ae_train_losses[-1]:.6f}\\n\"\n",
    "        f\"  Test Loss: {ae_test_losses[-1]:.6f}\\n\"\n",
    "        f\"  Улучшение: {ae_improvement:.1f}%\\n\\n\"\n",
    "        f\"VAE (30 эпох):\\n\"\n",
    "        f\"  Total Loss: {vae_losses[-1]:.6f}\\n\"\n",
    "        f\"  Recon Loss: {recon_losses[-1]:.6f}\\n\"\n",
    "        f\"  KLD: {kld_losses[-1]:.6f}\\n\"\n",
    "        f\"  Улучшение: {vae_improvement:.1f}%\\n\\n\"\n",
    "        f\"GAN (50 эпох):\\n\"\n",
    "        f\"  G Loss: {g_losses[-1]:.4f}\\n\"\n",
    "        f\"  D Loss: {d_losses[-1]:.4f}\\n\\n\"\n",
    "        f\"Diffusion (30 эпох):\\n\"\n",
    "        f\"  Loss: {diffusion_losses[-1]:.4f}\\n\"\n",
    "        f\"  Улучшение: {diff_improvement:.1f}%\\n\\n\"\n",
    "        f\"Autoregressive (20 эпох):\\n\"\n",
    "        f\"  Loss: {ar_losses[-1]:.4f}\\n\"\n",
    "        f\"  Улучшение: {ar_improvement:.1f}%\"\n",
    "    )\n",
    "    axes[1, 2].text(0.05, 0.5, summary_text, fontsize=9, \n",
    "                   verticalalignment='center', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('КРИВЫЕ ОБУЧЕНИЯ ГЕНЕРАТИВНЫХ МОДЕЛЕЙ (УВЕЛИЧЕННОЕ КОЛИЧЕСТВО ЭПОХ)', \n",
    "                fontsize=14, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ОБУЧЕНИЕ ЗАВЕРШЕНО!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Сохранение моделей\n",
    "    print(\"\\nСохранение моделей...\")\n",
    "    models_dict = {\n",
    "        'ae': ae_model,\n",
    "        'vae': vae_model,\n",
    "        'gan': gan_model,\n",
    "        'diffusion': diffusion_model,\n",
    "        'ar': ar_model\n",
    "    }\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        torch.save(model.state_dict(), f'{name}_model_extended.pth')\n",
    "        print(f\"Модель {name} сохранена в {name}_model_extended.pth\")\n",
    "    \n",
    "    # Анализ результатов\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"АНАЛИЗ РЕЗУЛЬТАТОВ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nAutoencoder:\")\n",
    "    print(f\"  Начальный loss: {ae_train_losses[0]:.6f}\")\n",
    "    print(f\"  Финальный loss: {ae_train_losses[-1]:.6f}\")\n",
    "    print(f\"  Улучшение: {ae_improvement:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nVariational Autoencoder:\")\n",
    "    print(f\"  Reconstruction улучшился с {recon_losses[0]:.6f} до {recon_losses[-1]:.6f}\")\n",
    "    print(f\"  KLD увеличился с {kld_losses[0]:.6f} до {kld_losses[-1]:.6f}\")\n",
    "    \n",
    "    print(f\"\\nGAN:\")\n",
    "    print(f\"  Минимальный Generator Loss: {min(g_losses):.4f} (эпоха {g_losses.index(min(g_losses))+1})\")\n",
    "    print(f\"  Минимальный Discriminator Loss: {min(d_losses):.4f} (эпоха {d_losses.index(min(d_losses))+1})\")\n",
    "    \n",
    "    print(f\"\\nDiffusion Model:\")\n",
    "    print(f\"  Начальный loss: {diffusion_losses[0]:.4f}\")\n",
    "    print(f\"  Финальный loss: {diffusion_losses[-1]:.4f}\")\n",
    "    print(f\"  Улучшение: {diff_improvement:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nAutoregressive Model:\")\n",
    "    print(f\"  Начальный loss: {ar_losses[0]:.4f}\")\n",
    "    print(f\"  Финальный loss: {ar_losses[-1]:.4f}\")\n",
    "    print(f\"  Улучшение: {ar_improvement:.1f}%\")\n",
    "    \n",
    "    return models_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Запуск основной функции\n",
    "    trained_models = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
